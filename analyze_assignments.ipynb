{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd8f2c-2aa0-483a-b307-7a9320eec17d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 0)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ddf550-1f4e-42a3-8d1c-df779ea25ebd",
   "metadata": {},
   "source": [
    "## Create a readable csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ea488a-e79a-44f4-8397-92aac1da2331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment_id = \"master_set\"\n",
    "experiment_id = \"prod_interface_test\"\n",
    "file_name = \"20220406-013800_hits_assignments.csv\"\n",
    "\n",
    "df = pd.read_csv(f\"./union_annotation/scripts/output/{experiment_id}/{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34df87-c7b5-4e6a-9b4d-728387a6780a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCAM_CONST = \"probably scam\"\n",
    "\n",
    "known_worker_ids = {\n",
    "    \"A2ALTKZTHJC8EF\": \"eran\",\n",
    "    \"A28IB5VURTLDJX\": \"valentina\",\n",
    "    \"A2MLICSDSPTW1T\": \"aviv_or_royi\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for hit_id, hit in df.groupby('HITId'):\n",
    "    hit_parsed = {\n",
    "        \"HitId\": hit_id,\n",
    "    }\n",
    "    \n",
    "    worker_count = 1\n",
    "    for _, assignment in hit.sort_values(\"WorkerId\").iterrows():\n",
    "        known_worker_id = known_worker_ids.get(assignment['WorkerId'], assignment['WorkerId'])\n",
    "        is_scam = known_worker_id == SCAM_CONST\n",
    "        if not is_scam:        \n",
    "            hit_parsed[f\"WorkerId_{worker_count}\"] = known_worker_id\n",
    "            answers = eval(assignment['answers'])\n",
    "            hit_parsed[f\"sentence1Text\"] = answers['taskData']['sentence1Text']\n",
    "            hit_parsed[f\"sentence2Text\"] = answers['taskData']['sentence2Text']        \n",
    "            hit_parsed[f\"exampleId\"] = answers['taskData']['exampleId']\n",
    "            hit_parsed[f\"chosenSentenceId_{worker_count}\"] = SCAM_CONST if is_scam else answers['chosenSentenceId'] \n",
    "            hit_parsed[f\"highlightedPhrases_{worker_count}\"] = SCAM_CONST if is_scam else answers['highlightedPhrases']\n",
    "            hit_parsed[f\"mergedText_{worker_count}\"] = SCAM_CONST if is_scam else answers['mergedText']        \n",
    "            hit_parsed[f\"feedbackText_{worker_count}\"] = SCAM_CONST if is_scam else answers['feedbackText']\n",
    "            hit_parsed[f\"skipped_{worker_count}\"] = SCAM_CONST if is_scam else answers['skipped']\n",
    "            worker_count += 1\n",
    "        \n",
    "    hit_parsed['count_workers'] = worker_count - 1\n",
    "        \n",
    "    rows.append(hit_parsed)\n",
    "\n",
    "mdf = pd.DataFrame(rows)\n",
    "mdf = mdf.reindex(sorted(mdf.columns), axis=1)\n",
    "mdf = mdf.sort_values('count_workers', ascending=False)\n",
    "mdf.to_csv(f\"{experiment_id}_analyzed_{file_name}\", index=False)\n",
    "mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf6ee5-fb02-4215-9517-cd347a46c968",
   "metadata": {},
   "source": [
    "## Score turkers response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e26924-3cf9-451c-b323-a4d8d17145ea",
   "metadata": {},
   "source": [
    "### Read turkers response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53315065-4d3e-4249-b785-23aca110676d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "turkers_df = pd.read_csv(\"prod_interface_test_analyzed - prod_interface_test_analyzed_20220406-013800_hits_assignments.csv\", skiprows=1)  # skiprows=1 because first row is file metadata (file_name)\n",
    "turkers_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9927237b-115f-4dd9-874f-795495e5bbcf",
   "metadata": {},
   "source": [
    "### Read master set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c31a63-0bcd-4402-a2de-3b824a5f437a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(\"master_set - master_set_consolidated_3.csv\")\n",
    "master_df = master_df.dropna(subset=['mergedText'])\n",
    "master_df = master_df.drop_duplicates(subset=['exampleId'])\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fa7d54-6451-4452-8e1b-19e39d6b0f87",
   "metadata": {},
   "source": [
    "### Create clusters of spans from merged text and highlighted phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51da9cb-b4c1-46ae-a274-8817939a8a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import numpy as np\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Any\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def find_new_spans(text, orig_text):\n",
    "    \"\"\"\n",
    "    Finds spans in text that weren't in the original text\n",
    "    \"\"\"\n",
    "\n",
    "    text = nlp(text)\n",
    "    orig_text = nlp(orig_text)\n",
    "    orig_text_words = {x.text: x for x in orig_text}\n",
    "    new_words = [word for word in text if word.text not in orig_text_words]\n",
    "    \n",
    "    new_words = sorted(new_words, key=lambda x: x.idx)\n",
    "    \n",
    "    new_spans = []\n",
    "    if any(new_words):\n",
    "        span = [new_words[0]]\n",
    "        for word in new_words[1:]:\n",
    "            if word.i - span[-1].i <= 2:\n",
    "                span.append(word)\n",
    "            else:\n",
    "                new_spans.append({\n",
    "                    \"span\": span,\n",
    "                    \"id\": len(new_spans)\n",
    "                })\n",
    "                span = [word]\n",
    "\n",
    "        if any(span):\n",
    "            new_spans.append({\n",
    "                    \"span\": span,\n",
    "                    \"id\": len(new_spans)\n",
    "                })\n",
    "                          \n",
    "    return new_spans\n",
    "\n",
    "def undo_tokenization(tokenized_text):\n",
    "    \"\"\"\n",
    "    tokenization can make it harder to compare texts, especially since it is not consistent (turkers might decide not to tokenize), try undoing some stuff like \"2 : 09 a . m .\"  or \"4 . 6\"\n",
    "    \"\"\"\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    def remove_space(text, start, end):\n",
    "        text_before = text[:start]\n",
    "        text_after = text[end:]\n",
    "        text_to_change = text[start:end].replace(\" \", \"\")\n",
    "                              \n",
    "        return f\"{text_before}{text_to_change}{text_after}\"\n",
    "    \n",
    "    NUMBERS_PATTERN = \"\\d+ [.:] \\d+\"\n",
    "    DOT_PATTERN = \"\\w \\.\"\n",
    "    PATTERNS = [NUMBERS_PATTERN, DOT_PATTERN]\n",
    "    \n",
    "    found = True\n",
    "    while found:\n",
    "        found = False\n",
    "        \n",
    "        for pattern in PATTERNS:\n",
    "            result = re.search(pattern, tokenized_text)\n",
    "            if result is not None:            \n",
    "                found = True\n",
    "                tokenized_text = remove_space(tokenized_text, result.start(), result.end())\n",
    "                break\n",
    "                              \n",
    "    return tokenized_text    \n",
    "    \n",
    "\n",
    "def create_highlighted_phrase_key(highlighted_phrase):\n",
    "    return f\"highlighted_span_{highlighted_phrase['id']}\"\n",
    "\n",
    "def create_new_span_key(new_span):\n",
    "    return f\"new_span_{new_span['id']}\"\n",
    "\n",
    "def equal_by_word(new_span, highlighted_phrase):\n",
    "    phrase_tokens = [token.text.lower() for token in highlighted_phrase['phrase_parsed']]\n",
    "    return any(token for token in new_span['span'] if token.text.lower() in phrase_tokens)\n",
    "\n",
    "\n",
    "def similarity_by_rouge(new_span, highlighted_phrase):\n",
    "    new_span_text = \" \".join([token.text for token in new_span['span'] if not token.is_stop and not token.is_punct])\n",
    "    highlighted_phrase_text = \" \".join([token.text for token in highlighted_phrase['phrase_parsed'] if not token.is_stop and not token.is_punct])\n",
    "    rouge_results = rouge_metric.compute(predictions=[new_span_text], references=[highlighted_phrase_text])\n",
    "    rouge_results = {key: value.mid.fmeasure * 100 for key, value in rouge_results.items()}\n",
    "\n",
    "    # print()\n",
    "    # print(new_span_text)\n",
    "    # print(highlighted_phrase_text)\n",
    "    # print(rouge_results)\n",
    "    \n",
    "    return rouge_results['rougeL']\n",
    "\n",
    "def keep_max_from_each_row(a):\n",
    "    \"\"\"\n",
    "    See https://stackoverflow.com/questions/20295046/numpy-change-max-in-each-row-to-1-all-other-numbers-to-0\n",
    "    \"\"\"\n",
    "    \n",
    "    is_max = a == a.max(axis=1)[:,None]\n",
    "    # we don't want 0s turning to 1 because the whole row is 0\n",
    "    is_not_zero = a != np.zeros(a.shape[0])[:,None]\n",
    "    return (is_max & is_not_zero).astype(int)\n",
    "\n",
    "def create_idx_per_item(items_and_funcs: List[Tuple[Any, Any]]):\n",
    "    \"\"\"\n",
    "    Create a unique running idx per item (necessary to create a matrix).\n",
    "    param items_and_funcs: list of tuples of the items and the functions used to create a unique key that can be used in a dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    item_to_idx = {}\n",
    "    idx_to_item = {}\n",
    "    for items, func in items_and_funcs:\n",
    "        for item in items:\n",
    "            idx_to_item[len(item_to_idx)] = item\n",
    "            item_to_idx[func(item)] = len(item_to_idx)\n",
    "        \n",
    "    return item_to_idx, idx_to_item\n",
    "    \n",
    "\n",
    "def create_clusters_from_new_and_highlighted_spans(highlighted_spans, new_spans):\n",
    "    \"\"\"\n",
    "    New spans are spans that were found in the merged sentence and not in the chosen sentence.\n",
    "    Highlighted phrases are those that the turker highlighted.\n",
    "    Create clusters combining the two\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    clusters = {}\n",
    "    if any(unique_span_to_idx):\n",
    "        unique_span_to_idx, idx_to_span = create_idx_per_item([(highlighted_spans, create_highlighted_phrase_key), (new_spans, create_new_span_key)])\n",
    "        \n",
    "        matrix = np.zeros((len(unique_span_to_idx), len(unique_span_to_idx)))\n",
    "\n",
    "        for new_span, highlighted_phrase in itertools.product(new_spans, highlighted_spans):\n",
    "            matrix[unique_span_to_idx[create_new_span_key(new_span)], unique_span_to_idx[create_highlighted_phrase_key(highlighted_phrase)]] = similarity_by_rouge(new_span, highlighted_phrase)\n",
    "\n",
    "        # Some words (e.g., miles) can show in multiple spans, but we want to match the span to the place it belongs best (max score)\n",
    "        matrix = keep_max_from_each_row(matrix)\n",
    "        n_components, labels = connected_components(csgraph=matrix, directed=False, return_labels=True)\n",
    "\n",
    "        for component in range(n_components):\n",
    "            items = []\n",
    "            for i, label in enumerate(labels):\n",
    "                if label == component:\n",
    "                    items.append(idx_to_span[i])\n",
    "\n",
    "            clusters[component] = {\n",
    "                \"items\": items\n",
    "            }\n",
    "            \n",
    "    return clusters\n",
    "    \n",
    "\n",
    "def align_new_and_highlighted_phrases(chosen_sent, merged_sent, highlighted_spans):\n",
    "    \"\"\"\n",
    "    Clusters new spans and the highlighted spans\n",
    "    \"\"\"\n",
    "    \n",
    "    higlighted_spans = deepcopy(highlighted_spans)\n",
    "    \n",
    "    new_spans = find_new_spans(undo_tokenization(merged_sent), undo_tokenization(chosen_sent))\n",
    "    \n",
    "    # create unique ids to each highlhighted phrase for matrix\n",
    "    for i, highlighted_phrase in enumerate(highlighted_spans):\n",
    "        highlighted_phrase['type'] = 'highlighted_phrase'\n",
    "        highlighted_phrase['id'] = i\n",
    "        highlighted_phrase['phrase_parsed'] = nlp(undo_tokenization(highlighted_phrase['phrase']))\n",
    "    \n",
    "    clusters = create_clusters()\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def test_align_new_and_highlighted_phrases__by_word():\n",
    "    \"\"\"\n",
    "    In this test, even a comparison by word will work because there is no overlap in spans in word\n",
    "    \"\"\"\n",
    "    \n",
    "    sent2_example = \"The quake occurred at 2 : 09 a . m . about 14 miles north - northeast of Healdsburg and had a depth of 1 . 2 miles .\"\n",
    "    merged_sent_example = \"A 4.6 magnitude earthquake occurred at 2 : 09 a . m . in Northern California overnight Thursday, centered about 14 miles north - northeast of Healdsburg at a depth of 1 . 2 miles, shaking residents from San Francisco to north of Sacramento.\"\n",
    "    highlighted_phrases_example = [{'phrase': '4 . 6 - magnitude', 'start': 2, 'end': 19, 'sentenceId': 1}, {'phrase': 'Northern California overnight Thursday', 'start': 39, 'end': 77, 'sentenceId': 1}, {'phrase': 'from San Francisco to north of Sacramento ', 'start': 98, 'end': 140, 'sentenceId': 1}]\t\n",
    "\n",
    "    result = align_new_and_highlighted_phrases(sent2_example, merged_sent_example, highlighted_phrases_example)\n",
    "    pprint(result)\n",
    "    assert len(result) == 3\n",
    "    \n",
    "test_align_new_and_highlighted_phrases__by_word()\n",
    "\n",
    "def test_align_new_and_highlighted_phrases__by_rouge():\n",
    "    \"\"\"\n",
    "    In this test, there is overlap in spans in word and puncuation\n",
    "    \"\"\"\n",
    "\n",
    "    sent1_example = \"A 4 . 6 - magnitude earthquake rattled Northern California overnight Thursday , shaking residents from San Francisco to north of Sacramento .\t\"\n",
    "    highlighted_phrases_example = [{\"phrase\":\"at 2 : 09 a . m\",\"type\":\"new\"},{\"phrase\":\"about 14 miles north - northeast of Healdsburg\",\"type\":\"new\"},{\"phrase\":\"had a depth of 1 . 2 miles \",\"type\":\"new\"}]\t\n",
    "    merged_sent_example = \"A 4 . 6 - magnitude earthquake, with a depth of 1.2 miles, rattled Northern California overnight at 2 : 09 a . m . on Thursday, about 14 miles north - northeast of Healdsburg , shaking residents from San Francisco to north of Sacramento .\"\n",
    "\n",
    "    result = align_new_and_highlighted_phrases(sent1_example, merged_sent_example, highlighted_phrases_example)\n",
    "    pprint(result)\n",
    "    assert len(result) == 3\n",
    "    \n",
    "# test_align_new_and_highlighted_phrases__by_rouge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea059dc-1f63-469a-814e-774fbe54e013",
   "metadata": {},
   "source": [
    "### Calculate turker's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f706125-9023-473e-a3b1-9568bdd0bd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses_json import dataclass_json\n",
    "from datasets import load_dataset, load_metric\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "ROUGE_SCORES = [\"rouge1\", \"rouge2\", \"rougeL\"]\n",
    "\n",
    "comparison_results = []\n",
    "num_turkers = 3\n",
    "\n",
    "# Variables to control the different algorithms tried (otherwise it takes a long time to run)\n",
    "align_new_and_highlighted_clusters = False\n",
    "compare_master_merge_to_turker_merge = False\n",
    "compare_entailing_to_entailed = True\n",
    "\n",
    "def remove_overlapping_words(text_to_remove_from, text_to_check_for_overlap):\n",
    "    return set(text_to_remove_from.split(\" \")).difference(set(text_to_check_for_overlap.split(\" \")))    \n",
    "\n",
    "def get_sentence_from_row(row, sentence_id) -> str:\n",
    "    return row['sentence1Text'] if sentence_id == 1 else row['sentence2Text']\n",
    "\n",
    "def compare_entailing_to_entailed(entailing_sent, entailed_sents: List[str], prefix: str):\n",
    "    \"\"\"\n",
    "    We would like to check the entailing sentence (e.g., merged sentence)\n",
    "    1. entails both base sentences.\n",
    "    2. does not hallucinate information not in the source sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    recall_results = []\n",
    "    for entailed_sent in entailed_sents:\n",
    "        rouge_results = rouge_metric.compute(predictions=[entailing_sent], references=[entailed_sent])\n",
    "        curr_recall_results = {f\"{prefix}_{key}_recall_entailing\": value.mid.recall for key, value in rouge_results.items()}\n",
    "        recall_results.append(curr_recall_results)\n",
    "\n",
    "    recall_results = pd.DataFrame(recall_results).mean().to_dict()\n",
    "    \n",
    "    # For calculating precision we want to join the two base sentences\n",
    "    rouge_results = rouge_metric.compute(predictions=[entailing_sent], references=[\" \".join(entailed_sents)])\n",
    "    precision_results = {f\"{prefix}_{key}_precision_hallucinating\": value.mid.precision for key, value in rouge_results.items()}\n",
    "        \n",
    "    return {**precision_results, **recall_results}\n",
    "\n",
    "def compare_merged_row(merged_row):\n",
    "    master_merge = merged_row['mergedText'] if isinstance(merged_row['mergedText'], str) else \"\"\n",
    "    master_chosen_sentence_id = merged_row[f'chosenSentenceId']\n",
    "    master_highlighted_phrases = eval(merged_row[f'highlightedPhrases']) if isinstance(merged_row[f'highlightedPhrases'], str) else []\n",
    "    master_orig = get_sentence_from_row(merged_row, master_chosen_sentence_id)\n",
    "    filtered_master_merge = remove_overlapping_words(master_merge, master_orig)    \n",
    "\n",
    "    master_clusters = None\n",
    "    if align_new_and_highlighted_clusters:\n",
    "        master_clusters = align_new_and_highlighted_phrases(master_orig, master_merge, master_highlighted_phrases)\n",
    "        \n",
    "    master_entailing_results = {}\n",
    "    if compare_entailing_to_entailed:\n",
    "        master_entailing_results = compare_entailing_to_entailed(master_merge, [merged_row['sentence1Text'], merged_row['sentence2Text']], prefix=\"master\")\n",
    "    \n",
    "    for i in range(num_turkers):\n",
    "        turker_merge = merged_row[f'mergedText_{i + 1}'] if isinstance(merged_row['mergedText'], str) else \"\"\n",
    "        turker_chosen_sentence_id = merged_row[f'chosenSentenceId_{i + 1}']\n",
    "        turker_highlighted_phrases = eval(merged_row[f'highlightedPhrases_{i + 1}']) if isinstance(merged_row[f'highlightedPhrases_{i + 1}'], str) else []\n",
    "        turker_orig = get_sentence_from_row(merged_row, merged_row[f'chosenSentenceId_{i + 1}'])\n",
    "        filtered_turker_merge = remove_overlapping_words(turker_merge, turker_orig)\n",
    "\n",
    "        align_new_and_highlighted_clusters_results = {}\n",
    "        if align_new_and_highlighted_clusters:\n",
    "            turker_clusters = align_new_and_highlighted_phrases(turker_orig, turker_merge, turker_highlighted_phrases)\n",
    "            len_cluster_diff = len(master_clusters) - len(turker_clusters)\n",
    "            master_highlighted_not_merged = len([cluster for cluster in master_clusters.values() if all(span.get('type') == 'highlighted_phrase' for span in cluster['items'])])\n",
    "            master_merged_not_highlighted = len([cluster for cluster in master_clusters.values() if all(span.get('type') != 'highlighted_phrase' for span in cluster['items'])])\n",
    "            turker_highlighted_not_merged = len([cluster for cluster in turker_clusters.values() if all(span.get('type') == 'highlighted_phrase' for span in cluster['items'])])\n",
    "            turker_merged_not_highlighted = len([cluster for cluster in turker_clusters.values() if all(span.get('type') != 'highlighted_phrase' for span in cluster['items'])])\n",
    "            turker_num_clusters = len(turker_clusters)\n",
    "\n",
    "            align_new_and_highlighted_clusters_results = {\n",
    "                \"master_clusters\": master_clusters,\n",
    "                \"turker_clusters\": turker_clusters,\n",
    "                \"len_cluster_diff\": len_cluster_diff,\n",
    "                \"master_highlighted_not_merged\": master_highlighted_not_merged,\n",
    "                \"master_merged_not_highlighted\": master_merged_not_highlighted,\n",
    "                \"turker_highlighted_not_merged\": turker_highlighted_not_merged,\n",
    "                \"turker_merged_not_highlighted\": turker_merged_not_highlighted,\n",
    "                \"turker_num_clusters\": turker_num_clusters\n",
    "            }\n",
    "\n",
    "\n",
    "        compare_master_merge_to_turker_merge_results = {}\n",
    "        if compare_master_merge_to_turker_merge:\n",
    "            # Compare the final merged sentences by rouge\n",
    "            rouge_results_filtered = rouge_metric.compute(predictions=[filtered_master_merge], references=[filtered_turker_merge])\n",
    "            rouge_results_filtered = {f\"{key}_filtered\": value.mid.fmeasure * 100 for key, value in rouge_results_filtered.items()}\n",
    "\n",
    "            # Compare the final merged sentences by rouge\n",
    "            rouge_results = rouge_metric.compute(predictions=[master_merge], references=[turker_merge])\n",
    "            rouge_results = {key: value.mid.fmeasure * 100 for key, value in rouge_results.items()}\n",
    "                        \n",
    "            compare_master_merge_to_turker_merge_results = {**rouge_results, **compare_master_merge_to_turker_merge_results}\n",
    "            compare_master_merge_to_turker_merge_results = {**rouge_results_filtered, **compare_master_merge_to_turker_merge_results}\n",
    "        \n",
    "        turker_entailing_results = {}\n",
    "        if compare_entailing_to_entailed:        \n",
    "            turker_entailing_results = compare_entailing_to_entailed(turker_merge, [merged_row['sentence1Text'], merged_row['sentence2Text']], prefix=\"turker\")\n",
    "            \n",
    "            # Compare turker results to master results\n",
    "            turker_compared_to_master_entailing_results = {}\n",
    "            for turker_entailing_key in turker_entailing_results:\n",
    "                key_without_prefix = \"_\".join(turker_entailing_key.split('_')[1:])\n",
    "                master_entailing_key = f\"master_{key_without_prefix}\"\n",
    "                turker_compared_to_master_entailing_results[f\"compare_{key_without_prefix}\"] = master_entailing_results[master_entailing_key] - turker_entailing_results[turker_entailing_key]\n",
    "                \n",
    "            turker_entailing_results = {**turker_compared_to_master_entailing_results, **turker_entailing_results}\n",
    "        \n",
    "        # get existing score if already started scoring\n",
    "        turker_score = merged_row.get(f'turker score {i + 1}')\n",
    "        turker_feedback = merged_row.get(f'turker feedback {i + 1}')\n",
    "        \n",
    "        \n",
    "        comparison_result = {\n",
    "            \"example_id\": merged_row['exampleId'],\n",
    "            \"sentence1Text\": merged_row['sentence1Text'],\n",
    "            \"sentence2Text\": merged_row['sentence2Text'],\n",
    "            \"turker_score\": turker_score,\n",
    "            \"turker_feedback\": turker_feedback,\n",
    "            \"turker_chosen_sentence_id\": turker_chosen_sentence_id,\n",
    "            \"turker_highlighted_phrases\": turker_highlighted_phrases,            \n",
    "            \"turker_merge\": turker_merge,\n",
    "            \"master_chosen_sentence_id\": master_chosen_sentence_id,\n",
    "            \"master_highlighted_phrases\": master_highlighted_phrases,\n",
    "            \"master_merge\": master_merge,\n",
    "            \"filtered_master_merge\": filtered_master_merge,\n",
    "            \"filtered_turker_merge\": filtered_turker_merge,\n",
    "        }\n",
    "\n",
    "        comparison_result = {**align_new_and_highlighted_clusters_results, **comparison_result}        \n",
    "        comparison_result = {**compare_master_merge_to_turker_merge_results, **comparison_result}                \n",
    "        comparison_result = {**master_entailing_results, **comparison_result}        \n",
    "        comparison_result = {**turker_entailing_results, **comparison_result}\n",
    "        comparison_results.append(comparison_result)\n",
    "        \n",
    "merged_df = pd.merge(turkers_df, master_df, left_on=\"exampleId\", right_on=\"exampleId\", suffixes=(\"_turkers\", \"_master\"))\n",
    "\n",
    "# Filter specific (for testing)\n",
    "# merged_df = merged_df[merged_df['exampleId'] == \"10_17ecbplus_2__10_2ecbplus_2\"]\n",
    "# merged_df = merged_df.iloc[:1]\n",
    "# merged_df = merged_df.iloc[1:2]\n",
    "\n",
    "# Run\n",
    "merged_df.apply(compare_merged_row, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea6a54-e2ca-4592-a5a3-7d68f75ad42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(comparison_results_copy[:int(size_to_fit)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374035a8-e71c-48a2-a802-21d72908b481",
   "metadata": {},
   "source": [
    "### Filter turkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a256cf-abcd-499f-8afc-1d330d32b0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "comparison_results_copy = comparison_results.copy()\n",
    "random.Random(42).shuffle(comparison_results_copy)\n",
    "size_to_fit = len(comparison_results_copy) # * 0.8\n",
    "\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results_copy[:int(size_to_fit)])\n",
    "# comparison_df = comparison_df.dropna(subset='turker_score')\n",
    "\n",
    "\n",
    "# metric = 'rouge1'\n",
    "# metric = 'turker_rouge1_precision_hallucinating'\n",
    "# metric = 'master_rouge1_precision_hallucinating'\n",
    "# metric = 'turker_rouge1_recall_entailing'\n",
    "# metric = 'master_rouge1_recall_entailing'\n",
    "# metric = 'compare_rouge1_precision_hallucinating'\n",
    "metric = 'compare_rouge1_recall_entailing'\n",
    "\n",
    "# should_filter_different_chosen_sentence = True\n",
    "should_filter_different_chosen_sentence = False\n",
    "# should_filter_empty_merge_both_sides = True\n",
    "should_filter_empty_merge_both_sides = False\n",
    "\n",
    "if should_filter_different_chosen_sentence:\n",
    "    comparison_df = comparison_df[comparison_df['turker_chosen_sentence_id'] == comparison_df['master_chosen_sentence_id']]\n",
    "\n",
    "if should_filter_empty_merge_both_sides:\n",
    "    comparison_df = comparison_df[(comparison_df['filtered_master_merge'] != comparison_df['filtered_turker_merge']) & (comparison_df['filtered_turker_merge'] == \"\")]\n",
    "\n",
    "# plot\n",
    "x = comparison_df[metric]\n",
    "y = comparison_df['turker_score']\n",
    "c = Counter(zip(x,y))\n",
    "s = [10*c[(xx,yy)] for xx,yy in zip(x,y)]\n",
    "plt.scatter(x, y, s=s)\n",
    "plt.show()\n",
    "\n",
    "print(comparison_df.shape)\n",
    "print(comparison_df['turker_score'].value_counts())\n",
    "\n",
    "comparison_df = comparison_df[abs(comparison_df[metric]) <= 0.1]\n",
    "# comparison_df = comparison_df[abs(comparison_df[metric]) > 0.1]\n",
    "# comparison_df = comparison_df[comparison_df['turker_score'] == 1.0]\n",
    "\n",
    "print(comparison_df.shape)\n",
    "print(comparison_df['turker_score'].value_counts())\n",
    "\n",
    "\n",
    "# comparison_df[['example_id', 'turker_score', metric, 'turker_feedback', 'len_cluster_diff', 'turker_clusters', 'turker_merge', 'filtered_turker_merge', 'master_clusters', 'master_merge', 'filtered_master_merge']]\n",
    "comparison_df['filtered_turker_merge'] = comparison_df['filtered_turker_merge'].apply(lambda x: sorted(x))\n",
    "comparison_df['filtered_master_merge'] = comparison_df['filtered_master_merge'].apply(lambda x: sorted(x))\n",
    "comparison_df[['example_id', 'turker_score', metric, 'turker_feedback', 'turker_merge', 'turker_chosen_sentence_id', 'filtered_turker_merge', 'master_merge', 'filtered_master_merge', 'turker_highlighted_phrases']].sort_values('example_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fced53-c57b-40f6-bb0b-73bd1fff7dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_cluster_key(cluster_key_and_value: Tuple[int, dict], prefix: str):\n",
    "    return f\"{prefix}_{cluster_key_and_value[0]}\"\n",
    "\n",
    "def create_cluster_key_master(cluster_key_and_value: Tuple[int, dict]):\n",
    "    return create_cluster_key(cluster_key_and_value, \"master\")\n",
    "\n",
    "def create_cluster_key_turker(cluster_key_and_value: Tuple[int, dict]):\n",
    "    return create_cluster_key(cluster_key_and_value, \"turker\")\n",
    "\n",
    "def similarity_by_highlighted_phrases(cluster_one, cluster_two):\n",
    "    \"\"\"\n",
    "    Compare clusters by the highlighted phrases, which should be consistent if they chose the same start sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    if cluster_one[1]['chosen_sentence_id'] != cluster_two[1]['chosen_sentence_id']:\n",
    "        # probably should default to the rouge of the two sentences\n",
    "        return -100  # TODO: change\n",
    "    \n",
    "    if cluster_one\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def create_clusters_from_clusters(master_clusters, turker_clusters):\n",
    "    \"\"\"\n",
    "    A good turker merge, although having the same new spans and highlighted phrases, might a different number of clusters than the master.\n",
    "    For example, master highlighted separately the phrases and turker highlighted continuously.\n",
    "    We want to create a joint cluster so we can properly compare if the turker missed anything or not.\n",
    "    \"\"\"\n",
    "    \n",
    "    cluster_to_idx, idx_to_cluster = create_idx_per_item([(master_clusters, create_cluster_key_master), (turker_clusters, create_cluster_key_turker)])\n",
    "    \n",
    "    similarity_func = similarity_by_highlighted_phrases\n",
    "        \n",
    "    clusters = {}\n",
    "    if any(cluster_to_idx):\n",
    "        matrix = np.zeros((len(cluster_to_idx), len(cluster_to_idx)))\n",
    "\n",
    "        for master_cluster, turker_cluster in itertools.product(master_clusters, turker_clusters):\n",
    "            matrix[cluster_to_idx[create_cluster_key_master(master_cluster)], cluster_to_idx[create_cluster_key_turker(turker_cluster)]] = similarity_func(master_cluster, turker_cluster)\n",
    "\n",
    "        matrix = keep_max_from_each_row(matrix)\n",
    "        n_components, labels = connected_components(csgraph=matrix, directed=False, return_labels=True)\n",
    "\n",
    "        for component in range(n_components):\n",
    "            items = []\n",
    "            for i, label in enumerate(labels):\n",
    "                if label == component:\n",
    "                    items.append(idx_to_cluster[i])\n",
    "\n",
    "            clusters[component] = {\n",
    "                \"items\": items\n",
    "            }\n",
    "            \n",
    "    return clusters\n",
    "\n",
    "row = comparison_df.iloc[1]\n",
    "for cluster in row['master_clusters'].values():\n",
    "    cluster['chosen_sentence_id'] = row['master_chosen_sentence_id']\n",
    "    cluster['type'] = 'master'\n",
    "\n",
    "for cluster in row['turker_clusters'].values():\n",
    "    cluster['chosen_sentence_id'] = row['turker_chosen_sentence_id']\n",
    "    cluster['type'] = 'turker'    \n",
    "\n",
    "def test_create_clusters_from_clusters():\n",
    "    \"\"\"\n",
    "    In this test, master annotated 3 and turker 1, but these are the same and should result in 1 merged cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    turker_clusters = {0: {'items': [{'span': [\"killing\", \"5\", \"while\", \"leaving\", \"dozens\", \"injured\"], 'id': 0}], 'chosen_sentence_id': 1, 'type': 'turker'}}\n",
    "    master_clusters = {0: {'items': [{'phrase': 'Five dead', 'type': 'highlighted_phrase', 'id': 0, 'phrase_parsed': \"Five dead\"}], 'chosen_sentence_id': 1.0, 'type': 'master'}, 1: {'items': [{'phrase': 'dozens injured', 'type': 'highlighted_phrase', 'id': 1, 'phrase_parsed': \"dozens injured\"}, {'span': [\"dozens\"], 'id': 1}], 'chosen_sentence_id': 1.0, 'type': 'master'}, 2: {'items': [{'span': [\"killing\"], 'id': 0}], 'chosen_sentence_id': 1.0, 'type': 'master'}}\n",
    "\n",
    "    result = create_clusters_from_clusters(list(row['turker_clusters'].items()), list(row['master_clusters'].items()))\n",
    "    pprint(result)\n",
    "    assert len(result) == 1\n",
    "    \n",
    "test_create_clusters_from_clusters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eran_nlp_38",
   "language": "python",
   "name": "eran_nlp_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
